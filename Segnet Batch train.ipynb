{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import sys\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from skimage.io import imsave\n",
    "from multiprocessing import Pool\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "import keras.backend as K\n",
    "from segnet import SegnetBuilder\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# \\ Globals ============\n",
    "PIC_H, PIC_W = 672, 832\n",
    "LABELS_NUMBER = 12\n",
    "PROCESSES = 30\n",
    "\n",
    "\n",
    "def w_categorical_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        nb_cl = len(weights)\n",
    "        final_mask = K.zeros_like(y_pred[:, 0])\n",
    "        y_pred_max = K.max(y_pred, axis=1, keepdims=True)\n",
    "        y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "        for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "            final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "        return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_filenames(datapath='lane_marking_examples'):\n",
    "    filenames = []\n",
    "    for top, dirs, files in os.walk(datapath):\n",
    "        filenames.extend([os.path.join(top, _file) for _file in files])\n",
    "    filenames.sort()\n",
    "\n",
    "    x_paths = [x for x in filenames if not x.endswith('bin.png')]\n",
    "    y_paths = [x for x in filenames if x.endswith('bin.png')]\n",
    "\n",
    "    return x_paths, y_paths\n",
    "\n",
    "\n",
    "def load_image(filepath, resize=True, pic_h=int(2710 / 4), pic_w=int(3384 / 4)):\n",
    "    if resize:\n",
    "        img = cv2.imread(filepath)\n",
    "    else:\n",
    "        img = cv2.resize(cv2.imread(y), (pic_h, pic_w), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    return filepath, img\n",
    "\n",
    "\n",
    "def load_one(pathes):\n",
    "    try:\n",
    "        xfilepath, yfilepath, file_ind = pathes\n",
    "\n",
    "        pic_h = 672\n",
    "        pic_w = 832\n",
    "\n",
    "        # Start: Labels conversion dict ===========================\n",
    "        labels = {\n",
    "            (0, 0, 0):       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            (8, 35, 142):    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            (43, 173, 180):  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            (153, 102, 153): [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            (234, 168, 160): [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            (192, 0, 0):     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            (8, 32, 128):    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            (12, 51, 204):   [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "            (70, 25, 100):   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            (14, 57, 230):   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            (75, 47, 190):   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            (255, 255, 255): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
    "        labels = dict([(k, np.array(v)) for k, v in labels.items()])\n",
    "        # End:   Labels conversion dict ===========================\n",
    "\n",
    "        img = cv2.resize(cv2.imread(xfilepath), (pic_w, pic_h), interpolation=cv2.INTER_NEAREST)\n",
    "        lbl = cv2.resize(cv2.imread(yfilepath), (pic_w, pic_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        mask = np.zeros((pic_h, pic_w, len(labels)))\n",
    "        for rgb, categorical_lbl in labels.items():\n",
    "            mask[(lbl == rgb).all(2)] = categorical_lbl\n",
    "\n",
    "        print(f'Loading image with index {str(file_ind).zfill(3)}', end='\\r')\n",
    "        return img, mask\n",
    "    except Exception as e:\n",
    "        print('!!! Exception', e)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_pair(task_queue, out_queue):\n",
    "    # Height and width to resize images\n",
    "    # Defaults: 2710, 3384\n",
    "    # Coef = 4: 677, 846 [OLD]\n",
    "    # Coef ~ 4: 672, 832 [Divisible by 32]\n",
    "\n",
    "    pic_h = 672\n",
    "    pic_w = 832\n",
    "\n",
    "    # Start: Labels conversion dict ===========================\n",
    "    labels = {\n",
    "        (0, 0, 0):       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        (8, 35, 142):    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        (43, 173, 180):  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        (153, 102, 153): [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        (234, 168, 160): [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        (192, 0, 0):     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        (8, 32, 128):    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        (12, 51, 204):   [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        (70, 25, 100):   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        (14, 57, 230):   [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        (75, 47, 190):   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        (255, 255, 255): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
    "    labels = dict([(k, np.array(v)) for k, v in labels.items()])\n",
    "    # End:   Labels conversion dict ===========================\n",
    "\n",
    "    while True:\n",
    "        xfilepath, yfilepath = task_queue.get()\n",
    "\n",
    "        if xfilepath == '':\n",
    "            break\n",
    "\n",
    "        img = cv2.resize(cv2.imread(xfilepath), (pic_w, pic_h), interpolation=cv2.INTER_NEAREST)\n",
    "        lbl = cv2.resize(cv2.imread(yfilepath), (pic_w, pic_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        mask = np.zeros((pic_h, pic_w, len(labels)))\n",
    "        for rgb, categorical_lbl in labels.items():\n",
    "            mask[(lbl == rgb).all(2)] = categorical_lbl\n",
    "\n",
    "        out_queue.put((img, mask))\n",
    "\n",
    "\n",
    "def predict_to_label(predictions):\n",
    "    labels = {\n",
    "        0:  (0, 0, 0),\n",
    "        1:  (8, 35, 142),\n",
    "        2:  (43, 173, 180),\n",
    "        3:  (153, 102, 153),\n",
    "        4:  (234, 168, 160),\n",
    "        5:  (192, 0, 0),\n",
    "        6:  (8, 32, 128),\n",
    "        7:  (12, 51, 204),\n",
    "        8:  (70, 25, 100),\n",
    "        9:  (14, 57, 230),\n",
    "        10: (75, 47, 190),\n",
    "        11: (255, 255, 255)}\n",
    "\n",
    "    # masks = np.zeros((len(predictions), PIC_H, PIC_W, 3))\n",
    "    masks = []\n",
    "    for ind, pred in enumerate(predictions):\n",
    "        pred = pred.reshape(PIC_H, PIC_W, 12)\n",
    "        pred = np.apply_along_axis(lambda x: np.argmax(x), axis=2, arr=pred)\n",
    "        h_pred = np.zeros((PIC_H, PIC_W, 3), dtype=np.uint8)\n",
    "        for argmax, rgb in labels.items():\n",
    "            h_pred[pred == argmax] = rgb\n",
    "\n",
    "        masks.append(h_pred)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def prepare_for_train(model_name=''):\n",
    "    if model_name == '':\n",
    "        model_name = f'Segnet {str(random.randint(0, 100)).zfill(3)}'\n",
    "\n",
    "    segnet = SegnetBuilder.build(model_name, PIC_W, PIC_H, 3, LABELS_NUMBER)\n",
    "\n",
    "    # wcc_loss = w_categorical_crossentropy()\n",
    "\n",
    "    segnet.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    return model_name, segnet\n",
    "\n",
    "\n",
    "def train_on_batch(sup_epoch: int, model_name: str, segnet: SegnetBuilder.build,\n",
    "                   x_data: np.ndarray, y_data: np.ndarray, load_from: int, load_to: int):\n",
    "    x_paths, y_paths = get_filenames()\n",
    "\n",
    "    load_total = load_to - load_from\n",
    "\n",
    "    x_paths = x_paths[load_from:load_to]\n",
    "    y_paths = y_paths[load_from:load_to]\n",
    "\n",
    "    _paths_prep = []\n",
    "    for i in range(len(x_paths)):\n",
    "        _paths_prep.append((x_paths[i], y_paths[i], load_from + i))\n",
    "\n",
    "    print('Starting pool')\n",
    "    with Pool(30) as p:\n",
    "        result = p.map(load_one, _paths_prep)\n",
    "    print('Pool closed', ' ' * 20)\n",
    "\n",
    "    index = 0\n",
    "    for _ in tqdm.tqdm_notebook(range(load_total)):\n",
    "        _x, _y = result[index]\n",
    "        if _x is not None:\n",
    "            x_data[index] = _x\n",
    "            y_data[index] = _y\n",
    "            index += 1\n",
    "\n",
    "    del result\n",
    "    gc.collect()\n",
    "\n",
    "    if load_total - index != 0:\n",
    "        print(f'Problems occured with {load_total - index} images')\n",
    "    else:\n",
    "        print(f'No problems occured during data loading')\n",
    "    # , data shape before {x_data.shape}; {y_data.shape}\n",
    "    # x_data: np.ndarray = x_data[:index]\n",
    "    # y_data: np.ndarray = y_data[:index]\n",
    "    # print(f'Data shape in result: {x_data.shape}; {y_data.shape}')\n",
    "\n",
    "    print('Data loaded, starting conversion...')\n",
    "    x_data = x_data / 255.\n",
    "    y_data = y_data.reshape(len(y_data), PIC_H * PIC_W, LABELS_NUMBER)\n",
    "\n",
    "    print(f'Data converted, starting \"{model_name}\" training at {sup_epoch} epoch batch {load_from} to {load_to}')\n",
    "\n",
    "    # print(x_data[0][0], '\\n\\n\\n', y_data[0].reshape(PIC_H, PIC_W, LABELS_NUMBER)[0])\n",
    "\n",
    "    # early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001,\n",
    "    #                            patience=3, verbose=1, mode='min')  # mode='auto' for val_acc\n",
    "    # checkpoint = ModelCheckpoint(f'models/{model_name} s[{sup_epoch}]b[{load_from}_{load_to}].hdf5',\n",
    "    #                              monitor='val_loss',\n",
    "    #                              verbose=1,\n",
    "    #                              save_best_only=True,\n",
    "    #                              mode='auto')\n",
    "    #\n",
    "    # callbacks = [early_stop, checkpoint]\n",
    "    callbacks = []\n",
    "\n",
    "    segnet.fit(x_data[:index], y_data[:index], batch_size=1, epochs=3,\n",
    "               verbose=1, validation_split=0.2, callbacks=callbacks)\n",
    "\n",
    "    _fname = f'models/{model_name} s[{sup_epoch}]b[{load_from}_{load_to}].hdf5'\n",
    "    segnet.save_weights(_fname)\n",
    "    print(f'Training ended, weights saved to {_fname}')\n",
    "\n",
    "    return segnet\n",
    "\n",
    "\n",
    "def check_prediction(segnet: SegnetBuilder.build, fname, pred_index=5):\n",
    "    x_paths, y_paths = get_filenames()\n",
    "    img, mask = load_one([x_paths[pred_index], y_paths[pred_index], 0])\n",
    "\n",
    "    print(f'Making predictions for image index {pred_index}')\n",
    "    predictions = segnet.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "    print('Converting prediction...')\n",
    "    human_pred = predict_to_label(predictions)[0]\n",
    "    human_mask = predict_to_label([mask])[0]\n",
    "\n",
    "    print(f'Saving result to {fname}')\n",
    "    imsave(fname, np.concatenate((human_mask, human_pred), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name, segnet = prepare_for_train()\n",
    "print('Segnet prepaired')\n",
    "batch_size = 800\n",
    "\n",
    "x_data = np.zeros((batch_size, PIC_H, PIC_W, 3), dtype=np.uint8)\n",
    "print('Xdata created')\n",
    "y_data = np.zeros((batch_size, PIC_H, PIC_W, LABELS_NUMBER), dtype=np.float64)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "batch_start = 0\n",
    "batch_end = 800\n",
    "\n",
    "segnet = train_on_batch(epoch, model_name, segnet, x_data, y_data, batch_start, batch_end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
